{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luciecharlottemagister/Documents/Cambridge/PhD/Projects/mil/mil_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import torch_geometric as pyg\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from mil.data.mnist import Bag, MNISTBags, OneHotMNISTBags, MNISTCollage, OneHotMNISTCollage\n",
    "from mil.utils import device, detach, human_format\n",
    "from mil.models import MILModel\n",
    "from mil.utils.visualize import print_one_hot_bag_with_attention, print_one_hot_bag, plot_attention_head, plot_bag, plot_one_hot_collage\n",
    "from mil.utils.stats import print_prediction_stats\n",
    "from mil.models.attention import WeightedAverageAttention, MultiHeadAttention\n",
    "from mil.models.set_transformer import SetTransformer, InducedSetTransformer, SAB, ISAB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST bags / MNIST collage\n",
    "\n",
    "This notebook trains models on variations of the *mnist-bags*, *multi-mnist-bags*, and *mnist-collage* datasets. \n",
    "The goal of this notebook is to see which models are able to overfit on these datasets.\n",
    "\n",
    "\n",
    "The following cell defines three variables, `DATASET`, `TARGET_NUMBERS` and `MODEL` which can be used to run different dataset/model configurations.\n",
    "\n",
    "`DATASET`:\n",
    "- `OneHotMNISTBags`: one-hot version of *mnist-bags*, where the dataset yields 10-dimensional one-hot encoded feature vectors directly (i.e. we are not yet working with MNIST digits)\n",
    "- `MNISTBags`: the *mnist-bags* dataset\n",
    "- `OneHotMNISTCollage`: one-hot version of *mnist-collage*\n",
    "- `MNISTCollage`: *mnist-collage* dataset\n",
    "\n",
    "`TARGET_NUMBERS`:\n",
    "- `0` corresponds to the *mnist-bags* dataset\n",
    "- `(0, 1)` corresponds to the *multi-mnist-bags* dataset\n",
    "\n",
    "`MODELS`:\n",
    "- `\"mean_pool\"`: simple baseline that uses mean pooling. Works neither dataset.\n",
    "- `\"max_pool\"`: simple baseline that uses max pooling. Works for *mnist-bags*, but not *multi-mnist-bags*.\n",
    "- `\"weighted_average_attention\"`: uses attention mechanism from \"Attention Based Deep Multiple Instance Learning\" paper which can only \"focus\" on one target number at a time. Works for *mnist-bags*, but not *multi-mnist-bags*.\n",
    "- `\"self_attention_mean_pooling\"`: uses a single transformer layer (self attention) followed by mean pooling. Works for both datasets.\n",
    "- `\"self_attention_max_pooling\"`: uses a single transformer layer (self attention) followed by max pooling. Works for both datasets (but better than mean pooling).\n",
    "\n",
    "\n",
    "Try changing `DATASET`, `TARGET_NUMBERS` and `MODELS` below and rerunning the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET = OneHotMNISTBags\n",
    "# DATASET = MNISTBags\n",
    "# DATASET = OneHotMNISTCollage\n",
    "DATASET = MNISTCollage\n",
    "\n",
    "# TARGET_NUMBERS = (0)\n",
    "TARGET_NUMBERS = (0, 1)\n",
    "\n",
    "# MODEL = \"mean_pool\"\n",
    "# MODEL = \"max_pool\"\n",
    "# MODEL = \"weighted_average_attention\"\n",
    "# MODEL = \"self_attention_mean_pool\"\n",
    "# MODEL = \"self_attention_max_pool\"\n",
    "# MODEL = \"set_transformer\"\n",
    "# MODEL = \"induced_set_transformer\"\n",
    "# MODEL = \"sab_max_pool\"\n",
    "# MODEL = \"isab_max_pool\"\n",
    "MODEL = \"gcn_max_pool\"\n",
    "# MODEL = \"dense_gcn_max_pool\"\n",
    "# MODEL = \"gat_max_pool\"\n",
    "\n",
    "# Only for collages\n",
    "COLLAGE_SIZE = 256\n",
    "MIN_DIST = 20\n",
    "\n",
    "NUM_DIGITS = 10\n",
    "if DATASET.__name__.startswith(\"OneHot\"):\n",
    "    FEATURE_SIZE = NUM_DIGITS # must be NUM_DIGITS due to one-hot encoding\n",
    "    HIDDEN_DIM = 10\n",
    "else:\n",
    "    FEATURE_SIZE = 64\n",
    "    HIDDEN_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_loader(train: bool = True):\n",
    "    \"\"\"Utility function to create a data loader for the dataset.\"\"\"\n",
    "    extra_kwargs = {}\n",
    "    if \"Collage\" in DATASET.__name__:\n",
    "        extra_kwargs[\"collage_size\"] = COLLAGE_SIZE\n",
    "        extra_kwargs[\"min_dist\"] = MIN_DIST\n",
    "    ds = DATASET(target_numbers=TARGET_NUMBERS, # target number\n",
    "                 num_digits=NUM_DIGITS, # sample from all 10 MNIST digits\n",
    "                 mean_bag_size=10, # mean bag length\n",
    "                 var_bag_size=2, # variance of bag length\n",
    "                 num_bags=250 if train else 100, # number of bags\n",
    "                 seed=1,\n",
    "                 train=train,\n",
    "                 **extra_kwargs)\n",
    "    loader = torch.utils.data.DataLoader(ds, batch_size=1, shuffle=train, collate_fn=lambda x: x[0])\n",
    "    return loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "The three parts of the MIL model are:\n",
    "1. **feature extractor**: extract a feature vector $z \\in \\mathbb{R}^D$ from each instance. In the case of the one-hot dataset, this is just the identity function. For the actual *mnist-bags* dataset, this is a CNN.\n",
    "2. **pooling**: a function $f : \\mathbb{R}^{N \\times D} \\to \\mathbb{R}^D$ that aggregates the $N$ feature vectors in the bag to a single feature vector.\n",
    "3. **classifier**: a function $g : \\mathbb{R}^D \\to \\mathbb{R}$ that transforms the aggregated feature vector into a binary classification prediction (we parameterise $g$ using a linear layer followed by a sigmoid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotFeatureExtractor(nn.Module):\n",
    "    def forward(self, bag: Bag):\n",
    "        # In the case of OneHotBags, the instances are already the features.\n",
    "        return bag.instances\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, feature_size: int):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Dropout2d(.1),\n",
    "            nn.Conv2d(10, 20, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Flatten(-3, -1),\n",
    "            nn.Dropout(.1),\n",
    "            nn.Linear(20 * 4 * 4, feature_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, bag: Bag):\n",
    "        return self.cnn(bag.instances)\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    \"\"\"GNN model.\n",
    "    \n",
    "    The layer parameter can be used to specify the type of GNN layer to use (e.g. pyg.nn.GCNConv, pyg.nn.GATConv, pyg.nn.DenseGCNConv).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_size: int, hidden_dim: int, layer=pyg.nn.GCNConv):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.gnn = pyg.nn.Sequential('x, connectivity', [\n",
    "            (layer(feature_size, hidden_dim), 'x, connectivity -> x'),\n",
    "            nn.ReLU(inplace=True),\n",
    "            (layer(hidden_dim, feature_size), 'x, connectivity -> x'),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ])\n",
    "\n",
    "    def forward(self, bag, features):\n",
    "        number_of_nodes = features.shape[0]\n",
    "        \n",
    "        # Assume complete graph\n",
    "        edge_index = np.array(nx.complete_graph(number_of_nodes).edges).T # complete (directed) graph\n",
    "        edge_index = np.concatenate([edge_index, edge_index[::-1]], axis=-1) # undirected graph\n",
    "\n",
    "        # Use distance-based edge weights (distance-based) to drop out edges\n",
    "        instance_locations = bag.instance_locations\n",
    "        if instance_locations is not None:\n",
    "            edge_weights = []\n",
    "            for i, (n1, n2) in enumerate(zip(edge_index[0], edge_index[1])):\n",
    "                eucl_dist = np.linalg.norm(n2 - n1)\n",
    "                edge_weights.append(eucl_dist)\n",
    "\n",
    "            # normalise\n",
    "            edge_weights = [float(i) / sum(edge_weights) for i in edge_weights]\n",
    "\n",
    "            # drop-out edge based on edge weights\n",
    "            edge_weights = np.array(edge_weights)\n",
    "            ei_1 = edge_index[0][(edge_weights > 0.5)]\n",
    "            ei_2 = edge_index[1][(edge_weights > 0.5)]\n",
    "            edge_index = np.array([ei_1, ei_2])\n",
    "\n",
    "        if self.layer == pyg.nn.DenseGCNConv:\n",
    "            # DenseGCNConv requires a dense adjacency matrix\n",
    "            adj = np.zeros((number_of_nodes, number_of_nodes))\n",
    "            for n1, n2 in zip(edge_index[0], edge_index[1]):\n",
    "                adj[n1][n2] = 1\n",
    "            \n",
    "            connectivity = torch.from_numpy(adj)\n",
    "        else:\n",
    "            edge_index = torch.from_numpy(edge_index)\n",
    "            connectivity = edge_index\n",
    " \n",
    "        batch = torch.tensor([0] * number_of_nodes)\n",
    "        x = self.gnn(features, connectivity)\n",
    "        x = pyg.nn.global_max_pool(x, batch)\n",
    "        return x\n",
    "\n",
    "class Aggregate(nn.Module):\n",
    "    \"\"\"Simple pooling layer for mean/max pooling.\"\"\"\n",
    "    def __init__(self, pool: str = \"mean\", dim: int = 0):\n",
    "        super().__init__()\n",
    "        self.pool = pool\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, features):\n",
    "        pool = getattr(torch, self.pool)\n",
    "        result = pool(features, dim=self.dim)\n",
    "        if self.pool == \"max\":\n",
    "            result = result.values\n",
    "        return result\n",
    "\n",
    "class Pooler(nn.Module):\n",
    "    \"\"\"Convienience wrapper for pooling layers.\"\"\"\n",
    "    def __init__(self, pooling_layer: nn.Module):\n",
    "        super().__init__()\n",
    "        self.pooling_layer = pooling_layer\n",
    "    \n",
    "    def forward(self, bag, features):\n",
    "        return self.pooling_layer(features)\n",
    "\n",
    "class Classifier(nn.Sequential):\n",
    "    def __init__(self, feature_size: int):\n",
    "        super().__init__(\n",
    "            nn.Linear(feature_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "class SqueezeUnsqueeze(nn.Module):\n",
    "    def __init__(self, module: nn.Module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.module(x.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "# Keep track of attention layer for visualization\n",
    "attention_layer = None\n",
    "\n",
    "# Define model\n",
    "feature_extractor = OneHotFeatureExtractor() if DATASET.__name__.startswith(\"OneHot\") else CNNFeatureExtractor(feature_size=FEATURE_SIZE)\n",
    "if MODEL == \"mean_pool\":\n",
    "    model = MILModel(feature_extractor=feature_extractor,\n",
    "                     pooler=Pooler(Aggregate(\"mean\")),\n",
    "                     classifier=Classifier(feature_size=FEATURE_SIZE))\n",
    "elif MODEL == \"max_pool\":\n",
    "    model = MILModel(feature_extractor=feature_extractor,\n",
    "                     pooler=Pooler(Aggregate(\"max\")),\n",
    "                     classifier=Classifier(feature_size=FEATURE_SIZE))\n",
    "elif MODEL == \"weighted_average_attention\":\n",
    "    attention_layer = WeightedAverageAttention(feature_size=FEATURE_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "    model = MILModel(feature_extractor=feature_extractor,\n",
    "                     pooler=Pooler(attention_layer),\n",
    "                     classifier=Classifier(feature_size=FEATURE_SIZE))\n",
    "elif MODEL == \"self_attention_mean_pool\":\n",
    "    attention_layer = MultiHeadAttention(feature_size=FEATURE_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "    pool = nn.Sequential(attention_layer, Aggregate(\"mean\"))\n",
    "    model = MILModel(feature_extractor=feature_extractor,\n",
    "                     pooler=Pooler(pool),\n",
    "                     classifier=Classifier(feature_size=FEATURE_SIZE))\n",
    "elif MODEL == \"self_attention_max_pool\":\n",
    "    attention_layer = MultiHeadAttention(feature_size=FEATURE_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "    pool = nn.Sequential(attention_layer, Aggregate(\"max\"))\n",
    "    model = MILModel(feature_extractor=feature_extractor,\n",
    "                     pooler=Pooler(pool),\n",
    "                     classifier=Classifier(feature_size=FEATURE_SIZE))\n",
    "elif MODEL == \"set_transformer\":\n",
    "    model = MILModel(feature_extractor=feature_extractor,\n",
    "                     pooler=Pooler(SetTransformer(dim_input=FEATURE_SIZE, num_outputs=1, dim_output=1, dim_hidden=HIDDEN_DIM, num_heads=1)),\n",
    "                     classifier=nn.Sigmoid())\n",
    "elif MODEL == \"induced_set_transformer\":\n",
    "    model = MILModel(feature_extractor=feature_extractor,\n",
    "                     pooler=Pooler(InducedSetTransformer(dim_input=FEATURE_SIZE, num_outputs=1, dim_output=1, dim_hidden=HIDDEN_DIM, num_heads=1, num_inds=32)),\n",
    "                     classifier=nn.Sigmoid())\n",
    "elif MODEL == \"sab_max_pool\":\n",
    "    model = MILModel(feature_extractor=feature_extractor,\n",
    "                     pooler=Pooler(nn.Sequential(SqueezeUnsqueeze(SAB(dim_in=FEATURE_SIZE, dim_out=FEATURE_SIZE, num_heads=1)), Aggregate(\"max\"))),\n",
    "                     classifier=Classifier(feature_size=FEATURE_SIZE))\n",
    "elif MODEL == \"isab_max_pool\":\n",
    "    model = MILModel(feature_extractor=feature_extractor,\n",
    "                     pooler=Pooler(nn.Sequential(SqueezeUnsqueeze(ISAB(dim_in=FEATURE_SIZE, dim_out=FEATURE_SIZE, num_heads=1, num_inds=32)), Aggregate(\"max\"))),\n",
    "                     classifier=Classifier(feature_size=FEATURE_SIZE))\n",
    "elif MODEL == \"gcn_max_pool\":\n",
    "    gnn = GNN(feature_size=FEATURE_SIZE, hidden_dim=HIDDEN_DIM, layer=pyg.nn.GCNConv)\n",
    "    model = MILModel(feature_extractor=feature_extractor,\n",
    "                     pooler=gnn,\n",
    "                     classifier=Classifier(feature_size=FEATURE_SIZE))\n",
    "elif MODEL == \"dense_gcn_max_pool\":\n",
    "    gnn = GNN(feature_size=FEATURE_SIZE, hidden_dim=HIDDEN_DIM, layer=pyg.nn.DenseGCNConv)\n",
    "    model = MILModel(feature_extractor=feature_extractor,\n",
    "                     pooler=gnn,\n",
    "                     classifier=Classifier(feature_size=FEATURE_SIZE))\n",
    "elif MODEL == \"gat_max_pool\":\n",
    "    gnn = GNN(feature_size=FEATURE_SIZE, hidden_dim=HIDDEN_DIM, layer=pyg.nn.GATConv)\n",
    "    model = MILModel(feature_extractor=feature_extractor,\n",
    "                     pooler=gnn,\n",
    "                     classifier=Classifier(feature_size=FEATURE_SIZE))\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model {MODEL}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss function and optimizer\n",
    "\n",
    "We use binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper code to evaluate on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This  <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "test_loader = make_data_loader(train=False)\n",
    "\n",
    "def test_loss_and_error(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.\n",
    "    total_error = 0.\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, bag in enumerate(loader):\n",
    "            bag = device(bag)\n",
    "            y = bag.bag_label.float()\n",
    "\n",
    "            # Calculate loss and metrics\n",
    "            y_pred = model(bag).squeeze()\n",
    "            loss = loss_function(y_pred, y)\n",
    "\n",
    "            predictions.append((detach(device(bag, \"cpu\")), y_pred.detach().cpu()))\n",
    "\n",
    "            error = 1. - ((y_pred > .5).float() == y).cpu().detach().float()\n",
    "            total_error += error\n",
    "            total_loss += loss.detach().cpu()\n",
    "    return total_loss / len(loader), total_error / len(loader), predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This  <class 'tuple'>\n",
      "Training model with 27.2K parameters\n",
      "Epoch:   0, loss: 0.6954, error: 0.4920, test_loss: 0.6929, test_error: 0.5000\n",
      "Epoch:   1, loss: 0.6947, error: 0.4880, test_loss: 0.6929, test_error: 0.5000\n",
      "Epoch:   2, loss: 0.6891, error: 0.4480, test_loss: 0.6715, test_error: 0.4200\n",
      "Epoch:   3, loss: 0.6153, error: 0.3160, test_loss: 0.6757, test_error: 0.3700\n",
      "Epoch:   4, loss: 0.5942, error: 0.3040, test_loss: 0.6346, test_error: 0.4000\n",
      "Epoch:   5, loss: 0.5451, error: 0.2800, test_loss: 0.6862, test_error: 0.4800\n",
      "Epoch:   6, loss: 0.5217, error: 0.2400, test_loss: 0.6588, test_error: 0.4200\n",
      "Epoch:   7, loss: 0.4598, error: 0.2240, test_loss: 0.7396, test_error: 0.4300\n",
      "Epoch:   8, loss: 0.4706, error: 0.2200, test_loss: 0.6759, test_error: 0.3400\n",
      "Epoch:   9, loss: 0.4631, error: 0.2280, test_loss: 0.6652, test_error: 0.3400\n",
      "Epoch:  10, loss: 0.4213, error: 0.1920, test_loss: 0.7198, test_error: 0.4000\n",
      "Epoch:  11, loss: 0.4040, error: 0.1920, test_loss: 0.7266, test_error: 0.4000\n",
      "Epoch:  12, loss: 0.3774, error: 0.1520, test_loss: 0.7227, test_error: 0.3900\n",
      "Epoch:  13, loss: 0.3646, error: 0.1520, test_loss: 0.8671, test_error: 0.3000\n",
      "Epoch:  14, loss: 0.3229, error: 0.1360, test_loss: 0.8648, test_error: 0.3700\n",
      "Epoch:  15, loss: 0.2999, error: 0.1400, test_loss: 0.8906, test_error: 0.4000\n"
     ]
    }
   ],
   "source": [
    "loader = make_data_loader(train=True)\n",
    "\n",
    "stats = []\n",
    "\n",
    "model.train()\n",
    "print(f\"Training model with {human_format(sum(p.numel() for p in model.parameters() if p.requires_grad))} parameters\")\n",
    "\n",
    "for epoch in range(40):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.\n",
    "    total_error = 0.\n",
    "    for bag in loader:\n",
    "        bag = device(bag)\n",
    "        y = bag.bag_label\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate loss and metrics\n",
    "        y_pred = model(bag).squeeze()\n",
    "        loss = loss_function(y_pred, y)\n",
    "\n",
    "        error = 1. - ((y_pred > .5).float() == y).cpu().detach().float()\n",
    "        total_error += error\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        total_loss += loss.detach().cpu()\n",
    "        # Step\n",
    "        optimizer.step()\n",
    "    \n",
    "    test_loss, test_error, _ = test_loss_and_error(model, test_loader)\n",
    "\n",
    "    stats.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"loss\": total_loss / len(loader),\n",
    "        \"error\": total_error / len(loader),\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_error\": test_error\n",
    "    })\n",
    "    print(\n",
    "        f\"Epoch: {epoch:3d}, loss: {total_loss/len(loader):.4f}, error: {total_error/len(loader):.4f}, test_loss: {test_loss:.4f}, test_error: {test_error:.4f}\")\n",
    "\n",
    "# Plot training and test loss/error\n",
    "stats = pd.DataFrame(stats)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(stats[\"epoch\"], stats[\"loss\"], label=\"train\")\n",
    "plt.plot(stats[\"epoch\"], stats[\"test_loss\"], label=\"test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Error\")\n",
    "plt.plot(stats[\"epoch\"], stats[\"error\"], label=\"train\")\n",
    "plt.plot(stats[\"epoch\"], stats[\"test_error\"], label=\"test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Test loss: 1.9366, test error: 0.2900\n",
      "\n",
      "                                 % acc    total    -pred    +pred     example\n",
      "  bags                              71      100       39       61     0 8 3 2 9 1 7\n",
      "  bags with 0s                      65       83       22       61     0 8 3 2 9 1 7\n",
      "  bags without 0s                  100       17       17        0     2 8 5 6 8 3 7 6\n",
      "  bags with 0s and 1s               72       70       19       51     0 8 3 2 9 1 7\n",
      "  bags with 0s and not 1s           23       13        3       10     6 6 8 3 0 3 6 6 9 4\n",
      "  bags with 1s                      76       80       29       51     0 8 3 2 9 1 7\n",
      "  bags without 1s                   50       20       10       10     2 8 5 6 8 3 7 6\n",
      "  bags with 1s and 0s               72       70       19       51     0 8 3 2 9 1 7\n",
      "  bags with 1s and not 0s          100       10       10        0     2 2 7 4 1 6 9 5 5 8 6\n",
      "  bags with 2 key instance(s)       80       46        9       37     \u001b[91m0\u001b[0m 0 5 8 2 \u001b[91m1\u001b[0m 9 2 0 6 5\n",
      "  bags with 3 key instance(s)      100        2        0        2     5 2 3 \u001b[91m1\u001b[0m 6 \u001b[91m0\u001b[0m \u001b[91m1\u001b[0m 3\n",
      "  bags with 4 key instance(s)      100        2        0        2     0 \u001b[91m0\u001b[0m \u001b[91m1\u001b[0m 5 2 5 3 9 \u001b[91m1\u001b[0m \u001b[91m0\u001b[0m 8\n",
      "- bags                              60       50       30       20     0 8 3 2 9 1 7\n",
      "- bags with 0s                      39       33       13       20     0 8 3 2 9 1 7\n",
      "- bags without 0s                  100       17       17        0     2 8 5 6 8 3 7 6\n",
      "- bags with 0s and 1s               50       20       10       10     0 8 3 2 9 1 7\n",
      "- bags with 0s and not 1s           23       13        3       10     6 6 8 3 0 3 6 6 9 4\n",
      "- bags with 1s                      66       30       20       10     0 8 3 2 9 1 7\n",
      "- bags without 1s                   50       20       10       10     2 8 5 6 8 3 7 6\n",
      "- bags with 1s and 0s               50       20       10       10     0 8 3 2 9 1 7\n",
      "- bags with 1s and not 0s          100       10       10        0     2 2 7 4 1 6 9 5 5 8 6\n",
      "+ bags                              82       50        9       41     \u001b[91m0\u001b[0m 0 5 8 2 \u001b[91m1\u001b[0m 9 2 0 6 5\n",
      "+ bags with 0s                      82       50        9       41     \u001b[91m0\u001b[0m 0 5 8 2 \u001b[91m1\u001b[0m 9 2 0 6 5\n",
      "+ bags with 0s and 1s               82       50        9       41     \u001b[91m0\u001b[0m 0 5 8 2 \u001b[91m1\u001b[0m 9 2 0 6 5\n",
      "+ bags with 1s                      82       50        9       41     \u001b[91m0\u001b[0m 0 5 8 2 \u001b[91m1\u001b[0m 9 2 0 6 5\n",
      "+ bags with 1s and 0s               82       50        9       41     \u001b[91m0\u001b[0m 0 5 8 2 \u001b[91m1\u001b[0m 9 2 0 6 5\n",
      "+ bags with 2 key instance(s)       80       46        9       37     \u001b[91m0\u001b[0m 0 5 8 2 \u001b[91m1\u001b[0m 9 2 0 6 5\n",
      "+ bags with 3 key instance(s)      100        2        0        2     5 2 3 \u001b[91m1\u001b[0m 6 \u001b[91m0\u001b[0m \u001b[91m1\u001b[0m 3\n",
      "+ bags with 4 key instance(s)      100        2        0        2     0 \u001b[91m0\u001b[0m \u001b[91m1\u001b[0m 5 2 5 3 9 \u001b[91m1\u001b[0m \u001b[91m0\u001b[0m 8\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_error, predictions = test_loss_and_error(model, test_loader)\n",
    "print(f\"Test loss: {test_loss:.4f}, test error: {test_error:.4f}\")\n",
    "\n",
    "print_prediction_stats(predictions, target_numbers=TARGET_NUMBERS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First 10 bags in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n"
     ]
    }
   ],
   "source": [
    "def visualize_prediction(bag, y_pred):\n",
    "    y = bag.bag_label\n",
    "    if DATASET == OneHotMNISTBags:\n",
    "        if isinstance(attention_layer, WeightedAverageAttention):\n",
    "            print_one_hot_bag_with_attention(bag, attention_layer.A, y_pred>.5)\n",
    "            print()\n",
    "        elif isinstance(attention_layer, MultiHeadAttention):\n",
    "            plt.figure()\n",
    "            plot_attention_head(bag, attention_layer.A[0])\n",
    "            plt.title(f\"Bag label: {y.item():.0f}, pred: {y_pred.item():.2f}\")\n",
    "        else:\n",
    "            print_one_hot_bag(bag, y_pred>.5)\n",
    "    elif DATASET == MNISTBags:\n",
    "        if isinstance(attention_layer, WeightedAverageAttention):\n",
    "            plot_bag(bag, y_pred=y_pred, attention=attention_layer.A.squeeze(-1))\n",
    "        elif isinstance(attention_layer, MultiHeadAttention):\n",
    "            plot_bag(bag, y_pred=y_pred)\n",
    "            plt.figure()\n",
    "            plot_attention_head(bag, attention_layer.A[0])\n",
    "            plt.title(f\"Bag label: {y.item():.0f}, pred: {y_pred.item():.2f}\")\n",
    "        else:\n",
    "            plot_bag(bag, y_pred=y_pred)\n",
    "    elif DATASET == OneHotMNISTCollage:\n",
    "        plt.figure()\n",
    "        plot_one_hot_collage(bag, y_pred=y_pred)\n",
    "        plt.title(f\"Bag label: {y.item():.0f}, pred: {y_pred.item():.2f}\")\n",
    "\n",
    "# Visualize first 10 bags\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for bag in itertools.islice(test_loader, 10):\n",
    "        bag = device(bag)\n",
    "        y = bag.bag_label.float()\n",
    "        y_pred = model(bag).squeeze(0)\n",
    "        visualize_prediction(bag, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First 10 mistakes in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n",
      "Do we get here\n",
      "do we get here 2\n",
      "do we get to this\n"
     ]
    }
   ],
   "source": [
    "# Visualize first 10 mistakes\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    for bag in test_loader:\n",
    "        if i == 10:\n",
    "            break\n",
    "        bag = device(bag)\n",
    "        y = bag.bag_label.float()\n",
    "        y_pred = model(bag).squeeze(0)\n",
    "        if ((y_pred > .5).float() != y).cpu().detach():\n",
    "            visualize_prediction(bag, y_pred)\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mil_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fa1507de89540ac93138f3c75d832673d05d41260750b9126b9b18c4372e249"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
