defaults:
  - _self_
  - dataset@dataset: mnist_collage
  - feature_extractor@model.feature_extractor: mnist
  - classifier@model.classifier: linear
  - optimizer@optimizer: adamw

settings:
  mnist:
    target_numbers: [0, 1]
    dist_predicate: "lambda dist: dist < 54"
  feature_size: 32
  hidden_dim: 10
  output_size: ${settings.feature_size}
  agg: max
  self_attention:
    num_heads: 1
  gnn:
    num_layers: 2
    special_loss: ""
  loss:
    use_pos_weight: true # whether to use pos_weight in BCEWithLogitsLoss (for imbalanced datasets)

name:
seed: # leave empty to use random seed
dataset: # will be overwritten
model:
  _target_: mil.models.MILModel
  feature_extractor: # will be overwritten
  pooler:
    _target_: torch_geometric.nn.Sequential
    input_args: "x, edge_index, edge_attr, pos"
    modules:
      - [
          { _target_: mil.utils.layers.Aggregate, agg: "${settings.agg}" },
          "x -> x",
        ]
  classifier: # will be overwritten
  # NOTE: our classifier is usually just a linear layer (no sigmoid because we use BCEWithLogitsLoss)
  logit_to_prob: # a function that converts logits to probabilities
    _target_: torch.sigmoid
    _partial_: true
loss: # loss function
  _target_: torch.nn.BCEWithLogitsLoss
save_epoch_freq: 10
num_epochs: 50
optimizer: # will be overwritten
device: cpu
wandb_id: # will be populated during training
job_type: train
group: ${name}
