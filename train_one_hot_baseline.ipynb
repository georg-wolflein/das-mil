{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from mil.data.mnist import Bag, OneHotMNISTBags\n",
    "from mil.utils import device\n",
    "from mil.models import MILModel\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot MNIST bags baseline\n",
    "\n",
    "We can train a model to overfit `OneHotMNISTBags` using max pooling. \n",
    "However, this only works for the *mnist-bags* variant, i.e. when there is one target number. \n",
    "In the *multi-mnist-bags* variant, this baseline fails (because it cannot \"focus\" on two target numbers simultaneously).\n",
    "\n",
    "Try changing `TARGET_NUMBERS` below and rerunning the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_NUMBERS = 9\n",
    "# TARGET_NUMBERS = (9, 7)\n",
    "\n",
    "def make_data_loader(train: bool = True):\n",
    "    \"\"\"Utility function to create a data loader for the dataset.\"\"\"\n",
    "    ds = OneHotMNISTBags(target_numbers=TARGET_NUMBERS, # target number\n",
    "                        min_instances_per_target=1, # 1 instance of a \"9\" suffices to label a bag as positive\n",
    "                        num_digits=10, # sample from all 10 MNIST digits\n",
    "                        mean_bag_size=10, # mean bag length\n",
    "                        var_bag_size=2, # variance of bag length\n",
    "                        num_bags=250, # number of bags\n",
    "                        seed=1,\n",
    "                        train=train)\n",
    "    loader = torch.utils.data.DataLoader(ds, batch_size=1, shuffle=train, collate_fn=lambda x: x[0])\n",
    "    return loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "This is a super simple baseline model for `OneHotBags`. The three parts of the MIL model are:\n",
    "1. **feature extractor**: no feature extractor, as the features are already the one-hot encoded digits (feature vectors are 10-dimensional)\n",
    "2. **max pooling**: simply retrieve maximum for each of the 10 dimensions in the feature vector\n",
    "3. **classifier**: linear layer from 10 to one unit followed by sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotFeatureExtractor(nn.Module):\n",
    "    def forward(self, bag: Bag):\n",
    "        # In the case of OneHotBags, the instances are already the features.\n",
    "        return bag.instances\n",
    "\n",
    "class SimplePooler(nn.Module):\n",
    "    \"\"\"Simple pooling layer for mean/max pooling.\"\"\"\n",
    "    def __init__(self, pool: str = \"mean\", dim: int = 0):\n",
    "        super().__init__()\n",
    "        self.pool = pool\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, bag: Bag, features):\n",
    "        pool = getattr(torch, self.pool)\n",
    "        result = pool(features, dim=self.dim)\n",
    "        if self.pool == \"max\":\n",
    "            result = result.values\n",
    "        return result\n",
    "\n",
    "class Classifier(nn.Sequential):\n",
    "    def __init__(self, hidden_dim: int):\n",
    "        super().__init__(\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "# Define model\n",
    "model = MILModel(feature_extractor=OneHotFeatureExtractor(),\n",
    "                 pooler=SimplePooler(\"max\"),\n",
    "                 classifier=Classifier(hidden_dim=10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss function and optimizer\n",
    "\n",
    "We use binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.6970, error: 0.4880\n",
      "Epoch: 1, loss: 0.6552, error: 0.3960\n",
      "Epoch: 2, loss: 0.6254, error: 0.3000\n",
      "Epoch: 3, loss: 0.6000, error: 0.2040\n",
      "Epoch: 4, loss: 0.5755, error: 0.1520\n",
      "Epoch: 5, loss: 0.5548, error: 0.1160\n",
      "Epoch: 6, loss: 0.5343, error: 0.0880\n",
      "Epoch: 7, loss: 0.5154, error: 0.0840\n",
      "Epoch: 8, loss: 0.4979, error: 0.0720\n",
      "Epoch: 9, loss: 0.4812, error: 0.0640\n",
      "Epoch: 10, loss: 0.4655, error: 0.0520\n",
      "Epoch: 11, loss: 0.4508, error: 0.0400\n",
      "Epoch: 12, loss: 0.4372, error: 0.0360\n",
      "Epoch: 13, loss: 0.4239, error: 0.0320\n",
      "Epoch: 14, loss: 0.4116, error: 0.0320\n",
      "Epoch: 15, loss: 0.3993, error: 0.0280\n",
      "Epoch: 16, loss: 0.3886, error: 0.0280\n",
      "Epoch: 17, loss: 0.3778, error: 0.0280\n",
      "Epoch: 18, loss: 0.3676, error: 0.0240\n",
      "Epoch: 19, loss: 0.3581, error: 0.0240\n"
     ]
    }
   ],
   "source": [
    "loader = make_data_loader(train=True)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(20):\n",
    "    total_loss = 0.\n",
    "    total_error = 0.\n",
    "    for bag in loader:\n",
    "        bag = device(bag)\n",
    "        y = bag.bag_label\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate loss and metrics\n",
    "        y_pred = model(bag).squeeze(0)\n",
    "        loss = loss_function(y_pred, y)\n",
    "\n",
    "        error = 1. - ((y_pred > .5).float() == y).cpu().detach().float()\n",
    "        total_error += error\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        total_loss += loss.detach().cpu()\n",
    "        # Step\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, loss: {total_loss/len(loader):.4f}, error: {total_error/len(loader):.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0: true, predicted bag label: 0, 0\n",
      "#1: true, predicted bag label: 0, 0\n",
      "#2: true, predicted bag label: 0, 0\n",
      "#3: true, predicted bag label: 1, 1\n",
      "#4: true, predicted bag label: 0, 0\n",
      "#5: true, predicted bag label: 1, 1\n",
      "#6: true, predicted bag label: 0, 0\n",
      "#7: true, predicted bag label: 0, 0\n",
      "#8: true, predicted bag label: 0, 0\n",
      "#9: true, predicted bag label: 1, 1\n",
      "Test loss: 0.3524, error: 0.0240\n"
     ]
    }
   ],
   "source": [
    "loader = make_data_loader(train=False)\n",
    "model.eval()\n",
    "\n",
    "total_loss = 0.\n",
    "total_error = 0.\n",
    "with torch.no_grad():\n",
    "    for i, bag in enumerate(loader):\n",
    "        bag = device(bag)\n",
    "        y = bag.bag_label.float()\n",
    "\n",
    "        # Calculate loss and metrics\n",
    "        y_pred = model(bag).squeeze(0)\n",
    "        loss = loss_function(y_pred, y)\n",
    "\n",
    "        error = 1. - ((y_pred > .5).float() == y).cpu().detach().float()\n",
    "        total_error += error\n",
    "        total_loss += loss.detach().cpu()\n",
    "\n",
    "        if i < 10:  # Print bag labels and instance labels for first 5 bags\n",
    "            print(f\"#{i}: true, predicted bag label: {bag.bag_label.float().item():.0f}, {(y_pred > .5).float().item():.0f}\")\n",
    "\n",
    "print(\n",
    "    f\"Test loss: {total_loss/len(loader):.4f}, error: {total_error/len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
