{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/georg/Projects/mil/mil_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/var/folders/75/85q4yvfd39z3s95j568dmh640000gn/T/ipykernel_30937/16867790.py:27: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  hydra.initialize(config_path=\"conf\")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import torch_geometric as pyg\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch_geometric as pyg\n",
        "import hydra\n",
        "from hydra.core.global_hydra import GlobalHydra\n",
        "import os\n",
        "os.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\n",
        "\n",
        "\n",
        "from mil.data.mnist import MNISTBags, OneHotMNISTBags, MNISTCollage, OneHotMNISTCollage\n",
        "from mil.utils import device, human_format, set_seed\n",
        "from mil.utils.visualize import print_one_hot_bag_with_attention, print_one_hot_bag, plot_attention_head, plot_bag, plot_one_hot_collage\n",
        "from mil.utils.stats import print_prediction_stats\n",
        "from mil.models.abmil import WeightedAverageAttention\n",
        "from mil.models.self_attention import MultiHeadSelfAttention\n",
        "from mil.models.distance_aware_self_attention import DistanceAwareSelfAttentionHead\n",
        "\n",
        "RESULTS_FILE = \"train.csv\"\n",
        "\n",
        "GlobalHydra().clear()\n",
        "hydra.initialize(config_path=\"conf\")\n",
        "cfg = hydra.compose(\"config.yaml\", overrides=[\"+experiment=mnist_bags\", \"+model=self_attention\"])\n",
        "\n",
        "set_seed(cfg.seed)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MNIST bags / MNIST collage\n",
        "\n",
        "This notebook trains models on variations of the *mnist-bags*, *multi-mnist-bags*, and *mnist-collage* datasets. \n",
        "The goal of this notebook is to see which models are able to overfit on these datasets.\n",
        "\n",
        "\n",
        "The following cell defines three variables, `DATASET`, `TARGET_NUMBERS` and `MODEL` which can be used to run different dataset/model configurations.\n",
        "\n",
        "`DATASET`:\n",
        "- `OneHotMNISTBags`: one-hot version of *mnist-bags*, where the dataset yields 10-dimensional one-hot encoded feature vectors directly (i.e. we are not yet working with MNIST digits)\n",
        "- `MNISTBags`: the *mnist-bags* dataset\n",
        "- `OneHotMNISTCollage`: one-hot version of *mnist-collage*\n",
        "- `MNISTCollage`: *mnist-collage* dataset\n",
        "\n",
        "`TARGET_NUMBERS`:\n",
        "- `0` corresponds to the *mnist-bags* dataset\n",
        "- `(0, 1)` corresponds to the *multi-mnist-bags* dataset\n",
        "\n",
        "`MODELS`:\n",
        "- `\"mean_pool\"`: simple baseline that uses mean pooling. Works neither dataset.\n",
        "- `\"max_pool\"`: simple baseline that uses max pooling. Works for *mnist-bags*, but not *multi-mnist-bags*.\n",
        "- `\"weighted_average_attention\"`: uses attention mechanism from \"Attention Based Deep Multiple Instance Learning\" paper which can only \"focus\" on one target number at a time. Works for *mnist-bags*, but not *multi-mnist-bags*.\n",
        "- `\"self_attention_mean_pooling\"`: uses a single transformer layer (self attention) followed by mean pooling. Works for both datasets.\n",
        "- `\"self_attention_max_pooling\"`: uses a single transformer layer (self attention) followed by max pooling. Works for both datasets (but better than mean pooling).\n",
        "\n",
        "\n",
        "Try changing `DATASET`, `TARGET_NUMBERS` and `MODELS` below and rerunning the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate datasets and loaders\n",
        "train_dataset = hydra.utils.instantiate(cfg.dataset.train)\n",
        "test_dataset = hydra.utils.instantiate(cfg.dataset.test)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=1, shuffle=True, collate_fn=lambda x: x[0], num_workers=0, pin_memory=False)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: x[0], num_workers=0, pin_memory=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the model\n",
        "\n",
        "The three parts of the MIL model are:\n",
        "1. **feature extractor**: extract a feature vector $z \\in \\mathbb{R}^D$ from each instance. In the case of the one-hot dataset, this is just the identity function. For the actual *mnist-bags* dataset, this is a CNN.\n",
        "2. **pooling**: a function $f : \\mathbb{R}^{N \\times D} \\to \\mathbb{R}^D$ that aggregates the $N$ feature vectors in the bag to a single feature vector.\n",
        "3. **classifier**: a function $g : \\mathbb{R}^D \\to \\mathbb{R}$ that transforms the aggregated feature vector into a binary classification prediction (we parameterise $g$ using a linear layer followed by a sigmoid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import typing\n",
        "from mil.utils import identity\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "class ABMIL(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the attention layer from the paper: \"Attention-Based Deep Multiple Instance Learning\", https://arxiv.org/pdf/1802.04712.pdf.\n",
        "\n",
        "    The attention layer is a weighted average of the features, where the weights are calculated by a neural network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_size: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(feature_size, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Softmax(dim=-2)\n",
        "        )\n",
        "\n",
        "    def forward(self, features):\n",
        "        H = features  # BxNxL\n",
        "\n",
        "        # Attention weights\n",
        "        A = self.attention(H)  # BxNx1\n",
        "\n",
        "        # Context vector (weighted average of the features)\n",
        "        M = torch.sum(A * H, dim=-2)  # BxL\n",
        "\n",
        "        self.A = A\n",
        "        return M\n",
        "\n",
        "class MILModel(nn.Module):\n",
        "    \"\"\"Structure of a multiple instance learning model.\n",
        "\n",
        "    The model consists of three parts:\n",
        "    1. A feature extractor that takes a bag and returns a Z-dimensional feature vector for each instance. A bag with N instances will thus be represented by a N x Z matrix.\n",
        "    2. A pooling function that takes the feature matrix and returns a single feature vector for the bag. (N x Z) -> (1 x Z)\n",
        "    3. A classifier that takes a bag and returns a single scalar value. (1 x Z) -> (1 x 1)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 feature_extractor: nn.Module,\n",
        "                 pooler: nn.Module,\n",
        "                 classifier: nn.Module,\n",
        "                 logit_to_prob: typing.Callable[[torch.Tensor], torch.Tensor] = identity):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.pooler = pooler\n",
        "        self.classifier = classifier\n",
        "        self.logit_to_prob = logit_to_prob\n",
        "\n",
        "    def forward(self, bag: Data):\n",
        "        features = self.feature_extractor(bag.x)\n",
        "        pooled = self.pooler(features, bag.edge_index, bag.edge_attr, bag.pos)\n",
        "        logit = self.classifier(pooled).squeeze(-1)\n",
        "        prob = self.logit_to_prob(logit)\n",
        "        return prob, logit\n",
        "\n",
        "class DefaultClassifier(nn.Module):\n",
        "    def __init__(self, input_dim: int):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        print(x.shape)\n",
        "\n",
        "class AdditiveClassifier(nn.Module):\n",
        "    def __init__(self, input_dim: int):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.linear(x).squeeze(-1).sum(-1)\n",
        "\n",
        "model = MILModel(\n",
        "    feature_extractor=hydra.utils.instantiate(cfg.model.feature_extractor),\n",
        "    pooler=pyg.nn.Sequential(\"x, edge_index, edge_attr\", [\n",
        "        (\n",
        "            ABMIL(feature_size=cfg.settings.feature_size, hidden_dim=cfg.settings.hidden_dim),\n",
        "            \"x -> x\"\n",
        "        )\n",
        "    ]),\n",
        "    classifier=DefaultClassifier(input_dim=cfg.settings.hidden_dim),\n",
        ")\n",
        "\n",
        "# model = hydra.utils.instantiate(cfg.model, _convert_=\"partial\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define loss function and optimizer\n",
        "\n",
        "We use binary cross-entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_function = nn.BCELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "def error_score(y_pred, y):\n",
        "    return 1. - ((y_pred > .5).float() == y).cpu().detach().float()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper code to evaluate on test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_loss_and_error(model, loader):\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.\n",
        "    total_error = 0.\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, bag in enumerate(loader):\n",
        "            bag = device(bag)\n",
        "            y = bag.y.float()\n",
        "\n",
        "            # Calculate loss and metrics\n",
        "            y_pred = model(bag).squeeze()\n",
        "            loss = loss_function(y_pred, y)\n",
        "\n",
        "            predictions.append((bag.cpu().detach(), y_pred.detach().cpu()))\n",
        "\n",
        "            error = error_score(y_pred, y)\n",
        "            total_error += error\n",
        "            total_loss += loss.detach().cpu()\n",
        "    return total_loss / len(loader), total_error / len(loader), predictions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model with 15.9K parameters\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "forward() takes 2 positional arguments but 4 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m \u001b[39m# Calculate loss and metrics\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m y_pred \u001b[39m=\u001b[39m model(bag\u001b[39m.\u001b[39;49mx, bag\u001b[39m.\u001b[39;49medge_index, bag\u001b[39m.\u001b[39;49medge_attr)\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     19\u001b[0m loss \u001b[39m=\u001b[39m loss_function(y_pred, y)\n\u001b[1;32m     21\u001b[0m error \u001b[39m=\u001b[39m error_score(y_pred, y)\n",
            "File \u001b[0;32m~/Projects/mil/mil_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 4 were given"
          ]
        }
      ],
      "source": [
        "stats = []\n",
        "\n",
        "model.train()\n",
        "print(f\"Training model with {human_format(sum(p.numel() for p in model.parameters() if p.requires_grad))} parameters\")\n",
        "\n",
        "for epoch in range(50):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0.\n",
        "    total_error = 0.\n",
        "    for bag in train_loader:\n",
        "        bag = device(bag)\n",
        "        y = bag.y\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculate loss and metrics\n",
        "        y_pred = model(bag.x, bag.edge_index, bag.edge_attr).squeeze()\n",
        "        loss = loss_function(y_pred, y)\n",
        "\n",
        "        error = error_score(y_pred, y)\n",
        "        total_error += error\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        total_loss += loss.detach().cpu()\n",
        "        # Step\n",
        "        optimizer.step()\n",
        "    \n",
        "    test_loss, test_error, _ = test_loss_and_error(model, test_loader)\n",
        "\n",
        "    stats.append({\n",
        "        \"epoch\": epoch,\n",
        "        \"loss\": total_loss / len(train_loader),\n",
        "        \"error\": total_error / len(train_loader),\n",
        "        \"test_loss\": test_loss,\n",
        "        \"test_error\": test_error\n",
        "    })\n",
        "    print(\n",
        "        f\"Epoch: {epoch:3d}, loss: {total_loss/len(train_loader):.4f}, error: {total_error/len(train_loader):.4f}, test_loss: {test_loss:.4f}, test_error: {test_error:.4f}\")\n",
        "\n",
        "# Plot training and test loss/error\n",
        "stats = pd.DataFrame(stats)\n",
        "float_cols = [col for col in stats.columns if col != \"epoch\"]\n",
        "stats[float_cols] = stats[float_cols].astype(float)\n",
        "stats.to_csv(RESULTS_FILE, index=False)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Loss\")\n",
        "plt.plot(stats[\"epoch\"], stats[\"loss\"], label=\"train\")\n",
        "plt.plot(stats[\"epoch\"], stats[\"test_loss\"], label=\"test\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(\"Error\")\n",
        "plt.plot(stats[\"epoch\"], stats[\"error\"], label=\"train\")\n",
        "plt.plot(stats[\"epoch\"], stats[\"test_error\"], label=\"test\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.legend()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loss, test_error, predictions = test_loss_and_error(model, test_loader)\n",
        "print(f\"Test loss: {test_loss:.4f}, test error: {test_error:.4f}\")\n",
        "\n",
        "print_prediction_stats(predictions, target_numbers=cfg.settings.mnist.target_numbers)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### First 10 bags in test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_dist_aware_attention(bag):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.suptitle(f\"Bag label: {y.item():.0f}, pred: {y_pred.item():.2f}\")\n",
        "    plt.subplot(141)\n",
        "    plt.title(\"dist\")\n",
        "    data = attention_layer.data\n",
        "    dist = pyg.utils.to_dense_adj(data.edge_index, edge_attr=data.edge_attr.squeeze(-1), max_num_nodes=data.num_nodes).squeeze(0)  # NxN\n",
        "    plot_attention_head(bag, dist, limit_range=False)\n",
        "    plt.subplot(142)\n",
        "    plt.title(\"A0\")\n",
        "    plot_attention_head(bag, attention_layer.A0)\n",
        "    plt.subplot(143)\n",
        "    plt.title(\"A\")\n",
        "    plot_attention_head(bag, attention_layer.A)\n",
        "\n",
        "def visualize_prediction(bag, y_pred):\n",
        "    y = bag.y\n",
        "    if DATASET == OneHotMNISTBags:\n",
        "        if isinstance(attention_layer, WeightedAverageAttention):\n",
        "            print_one_hot_bag_with_attention(bag, attention_layer.A, y_pred>.5)\n",
        "            print()\n",
        "        elif isinstance(attention_layer, MultiHeadAttention):\n",
        "            plt.figure()\n",
        "            plot_attention_head(bag, attention_layer.A[0])\n",
        "            plt.title(f\"Bag label: {y.item():.0f}, pred: {y_pred.item():.2f}\")\n",
        "        else:\n",
        "            print_one_hot_bag(bag, y_pred>.5)\n",
        "    elif DATASET == MNISTBags:\n",
        "        if isinstance(attention_layer, WeightedAverageAttention):\n",
        "            plot_bag(bag, y_pred=y_pred, attention=attention_layer.A.squeeze(-1))\n",
        "        elif isinstance(attention_layer, MultiHeadAttention):\n",
        "            plot_bag(bag, y_pred=y_pred)\n",
        "            plt.figure()\n",
        "            plot_attention_head(bag, attention_layer.A[0])\n",
        "            plt.title(f\"Bag label: {y.item():.0f}, pred: {y_pred.item():.2f}\")\n",
        "        else:\n",
        "            plot_bag(bag, y_pred=y_pred)\n",
        "    elif DATASET == OneHotMNISTCollage:\n",
        "        plt.figure()\n",
        "        plot_one_hot_collage(bag, y_pred=y_pred)\n",
        "        plt.title(f\"Bag label: {y.item():.0f}, pred: {y_pred.item():.2f}\")\n",
        "        if isinstance(attention_layer, DistanceAwareSelfAttentionHead):\n",
        "            plot_dist_aware_attention(bag)\n",
        "    elif DATASET == MNISTCollage:\n",
        "        plot_bag(bag, y_pred=y_pred, collage_size=COLLAGE_SIZE)\n",
        "        if isinstance(attention_layer, DistanceAwareSelfAttentionHead):\n",
        "            plot_dist_aware_attention(bag)\n",
        "\n",
        "# Visualize first 10 bags\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for bag in itertools.islice(test_loader, 10):\n",
        "        bag = device(bag)\n",
        "        y = bag.y.float()\n",
        "        y_pred = model(bag).squeeze(0)\n",
        "        visualize_prediction(bag, y_pred)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### First 10 mistakes in test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize first 10 mistakes\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    i = 0\n",
        "    for bag in test_loader:\n",
        "        if i == 10:\n",
        "            break\n",
        "        bag = device(bag)\n",
        "        y = bag.y.float()\n",
        "        y_pred = model(bag).squeeze(0)\n",
        "        if ((y_pred > .5).float() != y).cpu().detach():\n",
        "            visualize_prediction(bag, y_pred)\n",
        "            i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mil_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.14"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "0fa1507de89540ac93138f3c75d832673d05d41260750b9126b9b18c4372e249"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
